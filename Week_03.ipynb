{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1:** Create a simple DataFrame in Spark and explore some of the basic functions available."
      ],
      "metadata": {
        "id": "zvgn8ytAa9Ia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdbU92-SZTzo"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as SparkFuncs\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
        "\n",
        "# Main data\n",
        "data = [(\"James\", \"Sales\", 3000),\n",
        "        (\"Michael\", \"Sales\", 4600),\n",
        "        (\"Robert\", \"Sales\", 4100),\n",
        "        (\"Maria\", \"Finance\", 3000)]\n",
        "columns = [\"EmployeeName\", \"Department\", \"Salary\"]\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema=columns)\n",
        "print(\"Main DataFrame:\")\n",
        "dataframe.show()\n",
        "\n",
        "# Bonus data\n",
        "bonus_data = [(\"James\", 500), (\"Maria\", 3000)]\n",
        "bonus_columns = [\"EmployeeName\", \"Bonus\"]\n",
        "\n",
        "bonus_dataframe = spark.createDataFrame(bonus_data, schema=bonus_columns)\n",
        "print(\"Bonus DataFrame:\")\n",
        "bonus_dataframe.show()\n",
        "\n",
        "# Function 1: show()\n",
        "print(\"Show the first 2 rows:\")\n",
        "dataframe.show(2)\n",
        "\n",
        "# Function 2: orderBy()\n",
        "print(\"Ordered by salary (descending):\")\n",
        "dataframe.orderBy(\"Salary\", ascending=False).show()\n",
        "\n",
        "# Function 3: count()\n",
        "print(\"Number of rows:\", dataframe.count(), \"\\n\")\n",
        "\n",
        "# Function 4: groupBy()\n",
        "print(\"Group by department:\")\n",
        "dataframe.groupBy(\"Department\").count().show()\n",
        "\n",
        "# Function 5: printSchema()\n",
        "print(\"DataFrame's schema:\")\n",
        "dataframe.printSchema()\n",
        "\n",
        "# Function 6: select()\n",
        "print(\"Only show the EmployeeName and Department:\")\n",
        "dataframe.select(\"EmployeeName\", \"Department\").show()\n",
        "\n",
        "# Function 7: alias()\n",
        "print(\"Alias:\")\n",
        "dataframe.select(SparkFuncs.col(\"EmployeeName\").alias(\"Name\")).show()\n",
        "\n",
        "# Function 8: filter()\n",
        "print(\"Filtered salaries larger than 4400:\")\n",
        "dataframe.filter(dataframe[\"Salary\"] > 4400).show()\n",
        "\n",
        "# Function 9: join()\n",
        "print(\"dataframe and bonus_dataframe joined together:\")\n",
        "dataframe.join(bonus_dataframe, on=\"EmployeeName\", how=\"full\").show()\n",
        "\n",
        "# Function 10: agg()\n",
        "print(\"Salary average:\")\n",
        "dataframe.agg(SparkFuncs.avg(\"Salary\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2:** Use `filter`, `select`, `groupBy` operations to extract information from a data, and perform data aggregation to gain insight into the dataset using commands such as `mean`, `max`, `sum`."
      ],
      "metadata": {
        "id": "v4h-czRIwcgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as SparkFuncs\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
        "\n",
        "data = [(\"James\", \"Sales\", 3000),\n",
        "        (\"Michael\", \"Sales\", 4600),\n",
        "        (\"Robert\", \"Sales\", 4100),\n",
        "        (\"Maria\", \"Finance\", 3000)]\n",
        "columns = [\"EmployeeName\", \"Department\", \"Salary\"]\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "print(\"Only show the EmployeeName and Salary:\")\n",
        "dataframe.select(\"EmployeeName\", \"Salary\").show()\n",
        "\n",
        "print(\"Filtered salaries larger than 3000:\")\n",
        "dataframe.filter(dataframe[\"Salary\"] > 3000).show()\n",
        "\n",
        "print(\"Average, minimum, maximum, and sum of salaries in each departments:\")\n",
        "dataframe.groupBy(\"Department\").agg(\n",
        "    SparkFuncs.avg(\"Salary\").alias(\"Average\"),\n",
        "    SparkFuncs.min(\"Salary\").alias(\"Minimum\"),\n",
        "    SparkFuncs.max(\"Salary\").alias(\"Maximum\"),\n",
        "    SparkFuncs.sum(\"Salary\").alias(\"Sum\")\n",
        ").show()"
      ],
      "metadata": {
        "id": "GSWQtmprwcnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3:** Explore how to process complex data types in Spark DataFrames."
      ],
      "metadata": {
        "id": "YEQ-xC542nGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as SparkFuncs\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
        "\n",
        "data = [(\"James\", \"Sales\", 3000),\n",
        "        (\"Michael\", \"Sales\", 4600),\n",
        "        (\"Robert\", \"Sales\", 4100),\n",
        "        (\"Maria\", \"Finance\", 3000)]\n",
        "columns = [\"EmployeeName\", \"Department\", \"Salary\"]\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "print(\"Add SalaryBonus column to dataframe:\")\n",
        "dataframe = dataframe.withColumn(\"SalaryBonus\", dataframe[\"Salary\"] * 0.1)\n",
        "dataframe.show()\n",
        "\n",
        "print(\"Add TotalCompensation column to dataframe:\")\n",
        "dataframe = dataframe.withColumn(\"TotalCompensation\", dataframe[\"Salary\"] + dataframe[\"SalaryBonus\"])\n",
        "dataframe.show()\n",
        "\n",
        "print(\"Array usage example:\")\n",
        "dataframe.select(SparkFuncs.array(\"EmployeeName\", \"Salary\", \"SalaryBonus\")).show(truncate=False)"
      ],
      "metadata": {
        "id": "46MJoDvd2nO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4:** Implement a window function to calculate running totals or rankings."
      ],
      "metadata": {
        "id": "TffrbFcJ6XbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as SparkFuncs\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
        "\n",
        "data = [(\"James\", \"Sales\", 3000),\n",
        "        (\"Michael\", \"Sales\", 4600),\n",
        "        (\"Robert\", \"Sales\", 4100),\n",
        "        (\"Maria\", \"Finance\", 3000)]\n",
        "columns = [\"EmployeeName\", \"Department\", \"Salary\"]\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "window_spec = Window.partitionBy(\"Department\").orderBy(\"Salary\")\n",
        "dataframe.withColumn(\"Rank\", SparkFuncs.rank().over(window_spec)).show()"
      ],
      "metadata": {
        "id": "ymz9eNNH6Xpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5:**\n",
        "- Download a large dataset from [Kaggle](https://www.kaggle.com/) or another source.\n",
        "- Input the downloaded CSV data, then load and save the data into PySpark.\n",
        "- After the data has been successfully loaded using PySpark, manipulate the data to obtain the required information.\n",
        "\n",
        "[Dataset URL](https://www.kaggle.com/datasets/jahaidulislam/car-specification-dataset-1945-2020)"
      ],
      "metadata": {
        "id": "vWsvXuee6b2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as SparkFuncs\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
        "\n",
        "dataframe = spark.read.csv(\"/content/Car Dataset 1945-2020.csv\", header=True, inferSchema=True)\n",
        "dataframe.printSchema()\n",
        "\n",
        "dataframe = dataframe.select(\n",
        "    SparkFuncs.col(\"Make\").alias(\"Make\"),\n",
        "    SparkFuncs.col(\"Modle\").alias(\"Model\"),\n",
        "    SparkFuncs.col(\"Trim\").alias(\"Trim\"),\n",
        "    SparkFuncs.col(\"Year_from\").alias(\"Year From\"),\n",
        "    SparkFuncs.col(\"Year_to\").alias(\"Year To\"),\n",
        "    SparkFuncs.col(\"maximum_torque_n_m\").alias(\"Max Torque (n/m)\"),\n",
        "    SparkFuncs.col(\"number_of_cylinders\").alias(\"Cylinders\"),\n",
        "    SparkFuncs.col(\"capacity_cm3\").alias(\"Displacement (l)\"),\n",
        "    SparkFuncs.col(\"engine_hp\").alias(\"HP\"),\n",
        "    SparkFuncs.col(\"max_speed_km_per_h\").alias(\"Top Speed (km/h)\")\n",
        ").dropna()\n",
        "\n",
        "print(\"Top 10 Car Makers with the Most ModelsÂ Produced:\")\n",
        "dataframe.groupBy(\"Make\") \\\n",
        "  .count() \\\n",
        "  .withColumnRenamed(\"count\", \"Models Produced\") \\\n",
        "  .orderBy(\"Models Produced\", ascending=False) \\\n",
        "  .show(10)\n",
        "\n",
        "print(\"Top 10 cars with 10 cylinders or more:\")\n",
        "dataframe.filter((dataframe[\"Cylinders\"] > 10) & (dataframe[\"Year From\"] >= 2000)).show(10)\n",
        "\n",
        "print(\"Top 10 cars with the longest production span:\")\n",
        "dataframe.select(\"Make\", \"Model\", \"Year From\", \"Year To\") \\\n",
        "  .withColumn(\"Production Span (Years)\", dataframe[\"Year To\"] - dataframe[\"Year From\"]) \\\n",
        "  .groupBy(\"Make\", \"Model\", \"Year From\", \"Year To\") \\\n",
        "  .agg(SparkFuncs.max(\"Production Span (Years)\").alias(\"Production Span (Years)\")) \\\n",
        "  .orderBy(\"Production Span (Years)\", ascending=False) \\\n",
        "  .show(10)"
      ],
      "metadata": {
        "id": "OcGnBltx6cJI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
