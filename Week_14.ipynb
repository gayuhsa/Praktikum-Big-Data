{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3c03ce-14ae-4037-ae91-04bc5ab51188",
   "metadata": {},
   "source": [
    "## Introduction to Spark MLlib\n",
    "Spark MLlib is a scalable library for machine learning that integrates seamlessly with the Spark ecosystem. It supports a wide range of tasks, including regression, classification, clustering, and collaborative filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65bdc5ae-6aea-4e0e-afde-5b9b760ff739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/02 10:40:00 WARN Utils: Your hostname, user, resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlan0)\n",
      "25/12/02 10:40:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/02 10:40:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/02 10:40:32 WARN Instrumentation: [f6a7585d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/12/02 10:40:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/12/02 10:40:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"MLlib Example\").getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = [\"ID\", \"Feature\", \"Target\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=[\"Feature\"], outputCol=\"Features\")\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol=\"Features\", labelCol=\"Target\")\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f\"Coefficients: {model.coefficients}\")\n",
    "print(f\"Intercept: {model.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1bdc9c9-02bf-4cdb-832b-0f91d1389cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057925078082,4.087352265122887]\n",
      "Intercept: 11.568912722532378\n"
     ]
    }
   ],
   "source": [
    "# Practice: Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Example dataset\n",
    "data = [(1, Vectors.dense([2.0, 3.0]), 0), (2, Vectors.dense([1.0, 5.0]), 1), (3, Vectors.dense([2.5, 4.5]), 1), (4, Vectors.dense([3.0, 6.0]), 0)]\n",
    "columns = [\"ID\", \"Features\", \"Label\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"Features\", labelCol=\"Label\")\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Display coefficients and summary\n",
    "print(f\"Coefficients: {model.coefficients}\")\n",
    "print(f\"Intercept: {model.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc46d82-8ed5-4a6f-9773-5aeffc099387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
     ]
    }
   ],
   "source": [
    "# Practice: KMeans Clustering\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Example dataset\n",
    "data = [(1, Vectors.dense([1.0, 1.0])), (2, Vectors.dense([5.0, 5.0])), (3, Vectors.dense([10.0, 10.0])), (4, Vectors.dense([15.0, 15.0]))]\n",
    "columns = [\"ID\", \"Features\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train KMeans clustering model\n",
    "kmeans = KMeans(featuresCol=\"Features\", k=2)\n",
    "model = kmeans.fit(df)\n",
    "\n",
    "# Show cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(f\"Cluster Centers: {centers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a52c9-92f5-4827-a6da-6e6f9992815d",
   "metadata": {},
   "source": [
    "## Homework\n",
    "- Load a real-world dataset into Spark and prepare it for machine learning tasks.\n",
    "- Build a classification model using Spark MLlib and evaluate its performance.\n",
    "- Explore hyperparameter tuning using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fd56f-9b8f-4267-9fea-e2c3c73703d1",
   "metadata": {},
   "source": [
    "### Load a Real-World Dataset\n",
    "https://www.kaggle.com/datasets/yasserh/titanic-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ffb2c6-d3ad-44cb-99ff-76c04654b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MLlib Example\").getOrCreate()\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"yasserh/titanic-dataset\")\n",
    "\n",
    "df = spark.read.csv(f\"{dataset_path}/Titanic-Dataset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260fdb5-7529-4684-8a4e-f69515457011",
   "metadata": {},
   "source": [
    "### Clean the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39d507b8-a7d4-4710-9e2b-9dc792a6ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_median = df.approxQuantile(\"Age\", [0.5], 0)[0]\n",
    "df = df.fillna({\"Age\": age_median})\n",
    "\n",
    "mode_embarked = df.groupby(\"Embarked\").count().orderBy(\"count\", ascending=False).first()[0]\n",
    "df = df.fillna({\"Embarked\": mode_embarked})\n",
    "\n",
    "df = df.drop(\"Cabin\")\n",
    "\n",
    "indexer_sex = StringIndexer(inputCol=\"Sex\", outputCol=\"Sex_Index\")\n",
    "df = indexer_sex.fit(df).transform(df).drop(\"Sex\").withColumnRenamed(\"Sex_Index\", \"Sex\")\n",
    "\n",
    "indexer_embarked = StringIndexer(inputCol=\"Embarked\", outputCol=\"Embarked_Index\")\n",
    "df = indexer_embarked.fit(df).transform(df).drop(\"Embarked\").withColumnRenamed(\"Embarked_Index\", \"Embarked\")\n",
    "\n",
    "df = df.drop(\"Name\")\n",
    "df = df.drop(\"Ticket\")\n",
    "df = df.drop(\"Cabin\")\n",
    "df = df.drop(\"PassengerId\")\n",
    "\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec074273-0098-40d0-9d15-c585e88be161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+---+--------+\n",
      "|Survived|Pclass|Age |SibSp|Parch|Fare   |Sex|Embarked|\n",
      "+--------+------+----+-----+-----+-------+---+--------+\n",
      "|0       |3     |50.0|0    |0    |8.05   |0.0|0.0     |\n",
      "|0       |2     |25.0|1    |0    |26.0   |0.0|0.0     |\n",
      "|0       |3     |22.0|0    |0    |10.5167|1.0|0.0     |\n",
      "|0       |3     |28.0|0    |0    |7.7292 |0.0|2.0     |\n",
      "|0       |3     |28.0|0    |0    |8.05   |1.0|0.0     |\n",
      "+--------+------+----+-----+-----+-------+---+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea6315-2daa-48e7-b9cd-9aa5d69789ae",
   "metadata": {},
   "source": [
    "### Split the Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1e2fbb-5c69-4d69-8c44-9bc1b482aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\"]\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"Features\")\n",
    "\n",
    "train_df = assembler.transform(train_df)\n",
    "test_df = assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6c7a70-0893-4ebd-838f-2a3f60952236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+--------+---+--------+-----------------------------------+\n",
      "|Survived|Pclass|Age |SibSp|Parch|Fare    |Sex|Embarked|Features                           |\n",
      "+--------+------+----+-----+-----+--------+---+--------+-----------------------------------+\n",
      "|0       |1     |2.0 |1    |2    |151.55  |1.0|0.0     |[1.0,2.0,1.0,2.0,151.55,1.0,0.0]   |\n",
      "|0       |1     |18.0|1    |0    |108.9   |0.0|1.0     |[1.0,18.0,1.0,0.0,108.9,0.0,1.0]   |\n",
      "|0       |1     |19.0|3    |2    |263.0   |0.0|0.0     |[1.0,19.0,3.0,2.0,263.0,0.0,0.0]   |\n",
      "|0       |1     |21.0|0    |1    |77.2875 |0.0|0.0     |[1.0,21.0,0.0,1.0,77.2875,0.0,0.0] |\n",
      "|0       |1     |22.0|0    |0    |135.6333|0.0|1.0     |[1.0,22.0,0.0,0.0,135.6333,0.0,1.0]|\n",
      "+--------+------+----+-----+-----+--------+---+--------+-----------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a22fb50-317e-4839-8721-32225fa49739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+------+---+--------+---------------------------------+\n",
      "|Survived|Pclass|Age |SibSp|Parch|Fare  |Sex|Embarked|Features                         |\n",
      "+--------+------+----+-----+-----+------+---+--------+---------------------------------+\n",
      "|0       |1     |19.0|1    |0    |53.1  |0.0|0.0     |[1.0,19.0,1.0,0.0,53.1,0.0,0.0]  |\n",
      "|0       |1     |24.0|0    |0    |79.2  |0.0|1.0     |[1.0,24.0,0.0,0.0,79.2,0.0,1.0]  |\n",
      "|0       |1     |25.0|1    |2    |151.55|1.0|0.0     |[1.0,25.0,1.0,2.0,151.55,1.0,0.0]|\n",
      "|0       |1     |28.0|0    |0    |26.55 |0.0|0.0     |(7,[0,1,4],[1.0,28.0,26.55])     |\n",
      "|0       |1     |28.0|0    |0    |42.4  |0.0|0.0     |(7,[0,1,4],[1.0,28.0,42.4])      |\n",
      "+--------+------+----+-----+-----+------+---+--------+---------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "test_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843f7db-9f25-441b-a943-c16963130b3d",
   "metadata": {},
   "source": [
    "### Train the LogisticRegression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c7d5fe-b188-4c66-bd21-e0ebd2320742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------------------------+\n",
      "|Survived|prediction|probability                              |\n",
      "+--------+----------+-----------------------------------------+\n",
      "|0       |1.0       |[0.44415659912712796,0.5558434008728721] |\n",
      "|0       |1.0       |[0.3689044414680602,0.6310955585319398]  |\n",
      "|0       |1.0       |[0.0638702133835211,0.9361297866164789]  |\n",
      "|0       |1.0       |[0.4639518799977746,0.5360481200022253]  |\n",
      "|0       |1.0       |[0.45507557450082886,0.5449244254991712] |\n",
      "|0       |1.0       |[0.35785878445314984,0.6421412155468502] |\n",
      "|0       |1.0       |[0.4782961401296129,0.5217038598703871]  |\n",
      "|0       |0.0       |[0.6094746603286376,0.3905253396713624]  |\n",
      "|0       |0.0       |[0.6624269920490056,0.33757300795099443] |\n",
      "|0       |0.0       |[0.6229242657250683,0.3770757342749317]  |\n",
      "|0       |0.0       |[0.5698533666829934,0.43014663331700664] |\n",
      "|0       |0.0       |[0.6374540215798941,0.36254597842010594] |\n",
      "|0       |0.0       |[0.6306984093654345,0.36930159063456547] |\n",
      "|0       |0.0       |[0.7155016273071836,0.2844983726928164]  |\n",
      "|0       |0.0       |[0.7103053361760158,0.28969466382398423] |\n",
      "|0       |0.0       |[0.7539897210178569,0.24601027898214312] |\n",
      "|0       |0.0       |[0.6403219290228543,0.35967807097714566] |\n",
      "|0       |1.0       |[0.20799300066544915,0.7920069993345509] |\n",
      "|0       |0.0       |[0.7704949944804198,0.2295050055195802]  |\n",
      "|0       |0.0       |[0.7566880304683241,0.2433119695316759]  |\n",
      "|0       |0.0       |[0.8201773613933884,0.17982263860661163] |\n",
      "|0       |0.0       |[0.7971059136933882,0.20289408630661177] |\n",
      "|0       |0.0       |[0.7667774113773174,0.23322258862268264] |\n",
      "|0       |0.0       |[0.8586187289250552,0.1413812710749448]  |\n",
      "|0       |1.0       |[0.36176932666353795,0.638230673336462]  |\n",
      "|0       |1.0       |[0.201330165749067,0.798669834250933]    |\n",
      "|0       |0.0       |[0.9230242168730397,0.0769757831269603]  |\n",
      "|0       |0.0       |[0.9115225861302666,0.0884774138697334]  |\n",
      "|0       |0.0       |[0.9201170204594847,0.07988297954051526] |\n",
      "|0       |0.0       |[0.7987369120303693,0.20126308796963066] |\n",
      "|0       |1.0       |[0.30234241371869125,0.6976575862813088] |\n",
      "|0       |0.0       |[0.8855506936832478,0.11444930631675221] |\n",
      "|0       |1.0       |[0.2855768870130695,0.7144231129869305]  |\n",
      "|0       |0.0       |[0.8612900675187716,0.13870993248122843] |\n",
      "|0       |0.0       |[0.8606095824132255,0.1393904175867745]  |\n",
      "|0       |1.0       |[0.26521774210660154,0.7347822578933985] |\n",
      "|0       |0.0       |[0.8741219955679272,0.12587800443207275] |\n",
      "|0       |0.0       |[0.8737545252387285,0.12624547476127146] |\n",
      "|0       |1.0       |[0.3234242248717179,0.6765757751282822]  |\n",
      "|0       |0.0       |[0.878077001021502,0.12192299897849801]  |\n",
      "|0       |0.0       |[0.8777456934004167,0.12225430659958325] |\n",
      "|0       |0.0       |[0.8821913902011432,0.11780860979885677] |\n",
      "|0       |0.0       |[0.8895440448431992,0.11045595515680084] |\n",
      "|0       |0.0       |[0.8894831015676254,0.11051689843237456] |\n",
      "|0       |0.0       |[0.9168747544054757,0.08312524559452428] |\n",
      "|0       |0.0       |[0.9378190661706715,0.062180933829328544]|\n",
      "|0       |0.0       |[0.9087057361371779,0.09129426386282213] |\n",
      "|0       |0.0       |[0.871084551470675,0.12891544852932502]  |\n",
      "|0       |0.0       |[0.8710824497355357,0.12891755026446428] |\n",
      "|0       |1.0       |[0.37533901724195995,0.62466098275804]   |\n",
      "+--------+----------+-----------------------------------------+\n",
      "only showing top 50 rows\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(featuresCol=\"Features\", labelCol=\"Survived\")\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "predictions.select(\"Survived\", \"prediction\", \"probability\").show(50, truncate=False)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Survived\")\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380d512a-c3cc-4824-9398-e15eaa43ce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8087774294670851\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e266d-4d49-466e-bd41-c4749fec8e48",
   "metadata": {},
   "source": [
    "### Explore Hyperparameter Tuning Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8c46b4-83f1-4967-91e7-b282c977f65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated Model Accuracy: 0.7920585161964476\n"
     ]
    }
   ],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5)\n",
    "\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "cv_predictions = cv_model.transform(test_df)\n",
    "\n",
    "cv_accuracy = evaluator.evaluate(cv_predictions)\n",
    "\n",
    "print(f\"Cross-Validated Model Accuracy: {cv_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
